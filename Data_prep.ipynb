{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3019c8ae",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b607d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any Pip installs we need for this project\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install earthpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6738177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we need for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import earthpy as et\n",
    "import math\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3021d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep functions\n",
    "\n",
    "def get_arrest_data():\n",
    "    # I just picked the biggest dataset - this will take a few minutes to download. luckly you will only need to \n",
    "    # download this once\n",
    "    file_url = \"https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_statewide_2020_04_01.csv.zip\"\n",
    "    data_file = et.data.get_data(url=file_url)\n",
    "    fname = os.path.join(data_file, \"ca_statewide_2020_04_01.csv\")\n",
    "    return pd.read_csv(fname, on_bad_lines='skip')\n",
    "\n",
    "def shuffle_data(data, seed):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        np.random.shuffle(data.values)\n",
    "    except:\n",
    "        np.random.shuffle(data)\n",
    "    return(data)\n",
    "\n",
    "# splits data into training(2/3) and validation(1/3) dataframes\n",
    "def split_data(data_frame):\n",
    "    training_length = round(2/3 * len(data_frame))\n",
    "\n",
    "    training_df = data_frame[:training_length]\n",
    "    validation_df = data_frame[training_length:]\n",
    "\n",
    "    return training_df, validation_df\n",
    "\n",
    "# assuming that that yhat column is the last column in the dataset, this returns the features seperated from\n",
    "# the yhat column. for example to use this function: 'arrest_feat_df, arrest_yhat_df = split_data_yhat(df)'\n",
    "def split_data_yhat(df):\n",
    "    # This function will not work on our dataset - our yhat is 'arrest_made' column and it is not the \n",
    "    # last column of the dataset. so if someone needs this - you will have to re-write this function.\n",
    "    return df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for testing\n",
    "arrest_data_df = Data_prep.get_arrest_data()\n",
    "print(arrest_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d27eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
